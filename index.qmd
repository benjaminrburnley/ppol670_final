---
title: "Redistricting Project: Florida"
author: "Ben Burnley, Sean Conner, Gustavo Murillo Velazquez, & Katie Ward"
format: 
  html:
    self-contained: true
---
# Introduction 
```{r}
#| echo: false
#| message: false
#| warning: false
#| output: false

# libraries 
library(tidyverse)
library(tidymodels)
library(spatialsample)
library(sf)
library(patchwork)
library(janitor)

# data import 
dist_18 = st_read("data/fl_cong_2015_to_2021/fl_cong_2015_to_2021.shp") %>%  # 2015-2021 Congressional District Map 
  clean_names()
dist_22 = st_read("data/fl_cong_adopted_2022/P000C0109.shp") %>%  
# 2022 Congressional District Map 
 clean_names() %>% 
  st_transform(crs = st_crs(dist_18))
vote_18 = st_read("data/fl_vest_18/fl_vest_18.shp") %>% 
  clean_names() %>% 
  st_transform(crs = st_crs(dist_18))

# filter for valid geometries 
vote_18 = vote_18[st_is_valid(vote_18) == TRUE,]

# transform the crs of dist 20 to match 22 
dist_18 = st_transform(dist_18, crs = st_crs(dist_22))

# check crs 
st_crs(vote_18) == st_crs(dist_18)
st_crs(vote_18) == st_crs(dist_22)
```

This project examines the effect of the redistricting process on voter turnout. We use the change in maps from 2020 to 2022 in the state of Florida as a case study to examine this effect. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# plots comparing two maps 
plot1 = ggplot(dist_18)+
  geom_sf(fill = "white")+
  theme_void()+
  ggtitle("2015-2021 Map")

plot2 = ggplot(dist_22)+
  geom_sf(fill = "white")+
  theme_void()+
 ggtitle("2022 Map")

plot1 + plot2
```

```{r}
#| warning: false
#| messages: false
#| code-fold: true

# plot of precincts 
ggplot(vote_18)+
  geom_sf()+
  theme_void()
```


# How to use Redistricting Data Hub's API

```{python}
#| eval : false

#Import the four libraries needed to run the script. If you do not have these, you may need to install.
import pandas as pd
import requests
import io
from getpass import getpass
import json
import numpy as np}

#Below is the baseurl used to retrieve the list of datasets on the website.
baseurl = 'https://redistrictingdatahub.org/wp-json/download/list'

"""This function retrieves a list of all datasets on the RDH site. In order to run, you must be an API user and registered with the RDH site.
Inputs: username (string), password (string)
Optional Inputs: baseurl"""

def get_list(username, password, states, baseurl=baseurl):
    print('Retrieving list of datasets on RDH Website...')
    if type(states)!=type([]):
        states = [states]
    dfs = []
    for i in states:
        params = {}
        params['username'] = username
        params['password'] = password
        params['format'] = 'csv'
        params['states'] = i
        r = requests.get(baseurl, params=params)
        data = r.content
        try:
            df = pd.read_csv(io.StringIO(data.decode('utf-8')))
        except:
            print('There was an error retrieving the list of datasets, please check that you have the correct password and username')
            return 
        #display(df)
        dfs.append(df)
    df = pd.concat(dfs)
    return df
    
  def check_string(string_list, row):
    if len(string_list)==0:
        return True
    for i in string_list:
        if i not in row:
            return False
    return True
    
  def check_states(state_list, row):
    check_state = []
    if state_list == ['']:
        return True
    else:
        for i in state_list:
            if i == row:
                check_state.append(True)
                return True
            else:
                check_state.append(False)
        if any('True') in check_state:
            return True
        else:
            return False
                
def assign_fullname(state):
    state = state.lower()
    keys = ['al','ak','az','ar','ca','co','ct','de','fl',
              'ga','hi','id','il','in','ia','ks','ky','la','me',
              'md','ma','mi','mn','ms','mo','mt','ne',
              'nv','nh','nj','nm','ny','nc','nd','oh',
              'ok','or','pa','ri','sc','sd','tn','tx',
              'ut','vt','va','wa','wv','wi','wy']
    values = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida',
            'Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine',
            'Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska',
            'Nevada','New Hampshire','New Jersey','New Mexico','New York','North Carolina','North Dakota','Ohio',
            'Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas',
            'Utah','Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming']
    values = [i.lower() for i in values]
    dictionary = dict(zip(keys,values))
    for k, v in dictionary.items():
        if k == state:
            return v
        else:
            continue
    return state
    
def run_state_name(list_of_states):
    new_list = []
    for i in list_of_states:
        state = assign_fullname(i)
        new_list.append(state)
    return new_list
    
def get_inputs(run_no = 1):
    if run_no == 1:
        username = str(input('RDH Username or email: '))
        password = str(getpass(prompt='RDH Password: '))
    print('You can retrieve datasets by state by typing out the full state name or postal code abbreviation (e.g. "Alabama" or "alabama" or "AL" or "al").')
    print('If you would like data for multiple states, please separate by comma (e.g. "Wisconsin, mn").')
    print('Because of the limits of WordPress API, it can only retrieve a list of datasets for one state at a time (since many states have nearly 1,000 datasets), so if you are requesting data from multiple states this step may take several minutes, please be patient.')
    state = str(input('\nWhat state(s) do you want data for? Please separate by comma if multiple. '))
    state = [i.strip() for i in state.split(',')]
    state = [i.lower() for i in state]
    state = run_state_name(state)
    state = [i.lower() for i in state]

    print('\nYou can filter datasets in the state(s) you designated with the criteria listed below. All filter options are case insensitive.')
    print('You may search by year as YYYY for all years from 2010 to 2021.')
    print('You may search by dataset type with the following names: ACS5, CVAP, Projection, election results, voter file, incumbent, disag.')
    print('You may search by geogrpahy with the following: precinct, block, block group, census tract, vtd, county, state, aiannh, zctas, senate districts, legislative districts, congressional districts, house of represenative districts (or other district names for the SLDL or SLDU for a given state -- "districts" will retrieve all district boundaries).')
    print('***Please note that if you would like to retrieve the official redistricting dataset for your state, please use "official" (no quotations) in your query. Not all states will produce an offical dataset.')
    print('You may search by file type as CSV or SHP.')
    string = str(input('\nAny other filtering parameters? Please separate by comma (e.g. "election results, 2016, SHP" etc). '))
    string = [i.strip() for i in string.split(',')]
    string = [i.lower() for i in string]
    
    if run_no ==1:
        inputs = [username,password,state,string]
    else:
        inputs = [state,string]
    return inputs
    
'''This function extracts the data that meets input specifications to the current working directory. In order to run, you must be an API user and registered with the RDH site.
Inputs: username (string), password (string), state_name (string), add_string (list of strings)
Output: N/A'''
def get_data(run_no = 1,inputs=0,df = 0):
    #get list of datasets
    if (run_no == 1) or (run_no>1 and len(inputs)==1):
        inputs = get_inputs()
        username = inputs[0]
        password = inputs[1]
        state_name = inputs[2]
        add_string = inputs[3]
        df = get_list(username,password,state_name)
    else:
        username = inputs[0]
        password = inputs[1]
        inputs = get_inputs(run_no)
        state_name = inputs[0]
        add_string = inputs[1]
        df = get_list(username,password,state_name)
        inputs = [username,password,state_name,add_string]
    #read in the list of data
    for i in df.columns:
        if 'Filter by state found 0 states or unknown states' in i:
            print('*You did not specify the necessary states parameter.*')
            inputs = [inputs[0],inputs[1],'fill','fill']#,df_save]
            return inputs
    if df.shape[0]<10:
        print('\nYou either have an incorrect username/password or you are not a designated API user. To try again, please re-run.')
        print('If you continue to have problems or would like to become an API user, please email info@redistrictingdatahub.org')
        inputs = [0]
        return inputs
    params = {
    'username': username,
    'password': password}
    #subset the df by the additional string info
    df['Title_Format'] = df.apply(lambda x: ' '.join([x['Title'],x['Format']]),axis=1)
    df['Subset'] = df['Title_Format'].apply(lambda x: check_string(add_string,x.lower()))
    df = df[df['Subset']==True].copy()
    #take all of the urls in the subset df and split them to just get the baseurl of the dataset (no params)
    urls = list(df['URL'])
    new_urls = []
    id_dict = {}
    for i in urls:
        print(i)
        new = i.split('?')[0]
        dataset_id = i.split('&datasetid=')[1]
        id_dict.update({new:dataset_id})
        new_urls.append(new)
    titles = list(df['Title_Format'])
    if len(titles) == 0:
        print('\nThere are no datasets that currently meet your criteria. Please re-run with different criteria to extract data.')
        inputs = [inputs[0],inputs[1],'fill','fill']#,df_save]
        return inputs
    else:
        titles = ', '.join(titles)
        print('\nThe datasets to be extracted are: ', titles)
    cont = str(input('\nWould you like to extract these datasets to your current working directory? (Yes/No) '))
    ftype = list(df['Format'])
    data = dict(zip(new_urls,ftype))
    cont = cont.capitalize()
    
    if cont == 'Yes':
        counter = 1
        #iterate over all of the new urls and retrieve the data
        for i in new_urls:
            print('Retrieving', str(counter), 'of',str(len(new_urls)),' files')
            #get the data from the url and the params listed above
            params.update({'datasetid':id_dict.get(i)})
            response = requests.get(i,params)
            #get the file name of the dataset
            file_name = i.split('%2F')[-1]
            file_name = file_name.split('/')[-1]
            file_name_no_zip = file_name.split('.')[0]
            zipdot = '.'+file_name.split('.')[1]
            #because we have multiple datasets with the same name (for CSV and SHP), but we may want SHP or CSV, we need to make them unique filenames
            for k,v in data.items():
                if k == i:
                    dtype = '_'+v.lower()
                else:
                    continue
            #new filename
            if dtype in file_name_no_zip:
                dtype = ''
            file_name = file_name_no_zip+dtype+zipdot
            print('Retrieving ', file_name)
            #write the data
            file = open(file_name, "wb")
            file.write(response.content)
            file.close()
            counter = counter+1
        print('\nDone extracting datasets to current working directory.')
        print('Please re-run to extract additional data.')
    else:
        print('Data was not extracted. Please re-run if you would like to extract data.')
    return inputs
    
def re_run(run_no, inputs):
    run = str(input('\nWould you like to run a new extraction? (Yes/No) '))
    run = run.capitalize()
    if run == 'Yes':
        print('\nStarting a new extraction..')
        run_no = run_no+1
        inputs = get_data(run_no,inputs)
        re_run(run_no,inputs)
    else:
        print('\nThanks for using the RDH API tool! If you want to run again, please re-run the run() function (you will be asked for username/password again).')
        return
        
def check_versions():
    pd_check = str((pd.__version__))=='1.3.1'
    req_check = str(requests.__version__) == '2.25.1'
    np_check = str(np.__version__)=='1.20.3'
    if pd_check == False:
        print('WARNING: You do not have the correct version of pandas to run this script. This script may still work, but you may need to install pandas version 1.3.1 for this script to work.')
    if req_check == False:
        print('WARNING: You do not have the correct version of requests to run this script. This script may still work, but you may need to install requests version 2.25.1 for this script to work.')
    if np_check == False:
        print('WARNING: You do not have the correct version of numpy to run this script. This script may still work, but you may need to install numpy version 1.20.3 for this script to work.')
    
def run():
    check_versions()
    run_no = 1
    inputs = get_data()
    re_run(run_no,inputs)
    
run()
```


# Data Processing 

** SECTION ON DATA CLEANING AND MUTATION **

```{r}
#| warning: false
#| message: false

# loop for creating district 2018 objects 
districts_18 <- list()

for(i in 1:27){
districts_18[[i]] <- assign(paste0("dist_18_",i), filter(dist_18, district == i))
}

# loop for creating district 2022 objects
districts_22 <- list()

for(i in 1:28){
districts_22[[i]] <-  assign(paste0("dist_22_", i), filter(dist_22, district == i))
}

# create district columns for 2018 map 
for(i in 1:27){
  assign(paste0("within_18_",i), st_within(vote_18, districts_18[[i]], sparse = FALSE))
}

# create district columns for 2022 map 
for(i in 1:28){
  assign(paste0("within_22_",i), st_within(vote_18, districts_22[[i]], sparse = FALSE))
}

# collect columns
dist18_df = data.frame(dist_1 = within_18_1,
                   dist_2 = within_18_2,
                   dist_3 =  within_18_3,
                   dist_4 = within_18_4,
                   dist_5 = within_18_5,
                   dist_6 = within_18_6,
                   dist_7 = within_18_7,
                   dist_8 = within_18_8,
                   dist_9 = within_18_9,
                   dist_10 = within_18_10,
                   dist_11 = within_18_11,
                   dist_12 = within_18_12,
                   dist_13 = within_18_13,
                   dist_14 = within_18_14,
                   dist_15 = within_18_15,
                   dist_16 = within_18_16,
                   dist_17 = within_18_17,
                   dist_18 = within_18_18,
                   dist_19 = within_18_19,
                   dist_20 = within_18_20,
                   dist_21 = within_18_21,
                   dist_22 = within_18_22,
                   dist_23 = within_18_23,
                   dist_24 = within_18_24,
                   dist_25 = within_18_25,
                   dist_26 = within_18_26,
                   dist_27 = within_18_27)

data_18 = dist18_df %>% 
  mutate(district_18 = case_when(
    dist_1 == TRUE ~ "District 1",
    dist_2 == TRUE ~ "District 2",
    dist_3 == TRUE ~ "District 3", 
    dist_4 == TRUE ~ "District 4",
    dist_5 == TRUE ~ "District 5", 
    dist_6 == TRUE ~ "District 6", 
    dist_7 == TRUE ~ "District 7", 
    dist_8 == TRUE ~ "District 8",
    dist_9 == TRUE ~ "District 9", 
    dist_10 == TRUE ~ "District 10", 
    dist_11 == TRUE ~ "District 11", 
    dist_12 == TRUE ~ "District 12", 
    dist_13 == TRUE ~ "District 13", 
    dist_14 == TRUE ~ "District 14", 
    dist_15 == TRUE ~ "District 15", 
    dist_16 == TRUE ~ "District 16",
    dist_17 == TRUE ~ "District 17", 
    dist_18 == TRUE ~ "District 18", 
    dist_19 == TRUE ~ "District 19",
    dist_20 == TRUE ~ "District 20", 
    dist_21 == TRUE ~ "District 21", 
    dist_22 == TRUE ~ "District 22",
    dist_23 == TRUE ~ "District 23", 
    dist_24 == TRUE ~ "District 24", 
    dist_25 == TRUE ~ "District 25", 
    dist_26 == TRUE ~ "District 26",
    dist_27 == TRUE ~ "District 27", 
    TRUE ~ "Split"
  ))

dist22_df = data.frame(dist_1 = within_22_1,
                   dist_2 = within_22_2,
                   dist_3 =  within_22_3,
                   dist_4 = within_22_4,
                   dist_5 = within_22_5,
                   dist_6 = within_22_6,
                   dist_7 = within_22_7,
                   dist_8 = within_22_8,
                   dist_9 = within_22_9,
                   dist_10 = within_22_10,
                   dist_11 = within_22_11,
                   dist_12 = within_22_12,
                   dist_13 = within_22_13,
                   dist_14 = within_22_14,
                   dist_15 = within_22_15,
                   dist_16 = within_22_16,
                   dist_17 = within_22_17,
                   dist_18 = within_22_18,
                   dist_19 = within_22_19,
                   dist_20 = within_22_20,
                   dist_21 = within_22_21,
                   dist_22 = within_22_22,
                   dist_23 = within_22_23,
                   dist_24 = within_22_24,
                   dist_25 = within_22_25,
                   dist_26 = within_22_26,
                   dist_27 = within_22_27,
                   dist_28 = within_22_28)

data_22 = dist22_df %>% 
  mutate(district_22 = case_when(
    dist_1 == TRUE ~ "District 1",
    dist_2 == TRUE ~ "District 2",
    dist_3 == TRUE ~ "District 3", 
    dist_4 == TRUE ~ "District 4",
    dist_5 == TRUE ~ "District 5", 
    dist_6 == TRUE ~ "District 6", 
    dist_7 == TRUE ~ "District 7", 
    dist_8 == TRUE ~ "District 8",
    dist_9 == TRUE ~ "District 9", 
    dist_10 == TRUE ~ "District 10", 
    dist_11 == TRUE ~ "District 11", 
    dist_12 == TRUE ~ "District 12", 
    dist_13 == TRUE ~ "District 13", 
    dist_14 == TRUE ~ "District 14", 
    dist_15 == TRUE ~ "District 15", 
    dist_16 == TRUE ~ "District 16",
    dist_17 == TRUE ~ "District 17", 
    dist_18 == TRUE ~ "District 18", 
    dist_19 == TRUE ~ "District 19",
    dist_20 == TRUE ~ "District 20", 
    dist_21 == TRUE ~ "District 21", 
    dist_22 == TRUE ~ "District 22",
    dist_23 == TRUE ~ "District 23", 
    dist_24 == TRUE ~ "District 24", 
    dist_25 == TRUE ~ "District 25", 
    dist_26 == TRUE ~ "District 26",
    dist_27 == TRUE ~ "District 27", 
    dist_28 == TRUE ~ "District 28",
    TRUE ~ "Split"
  ))

# combine datasets and create redrawn column
data_both = data_18 %>% 
  bind_cols(data_22) %>% 
  mutate(redrawn = if_else(district_18 == district_22, FALSE, TRUE)) %>% 
  dplyr::select(district_18, district_22, redrawn)

# data frame with column indicating whether a precinct was drawn into a new district
data = vote_18 %>% 
  bind_cols(data_both)
```

# Visualizations 

** SECTION ON DATA VISUALIZATIONS ** 
```{r}
#| warning: false
#| message: false
#| code-fold: true

# map of overlapping geometries 
ggplot()+
  geom_sf(data = dist_18, color = "red", alpha = 0, lwd = 1) +
  geom_sf(data = dist_22, color = "blue", alpha = 0, lwd = 0.5) + 
  theme_void()+
  labs(
    title = "Overlay of Old and New Districts ",
    subtitle = "Red lines are old borders, purple lines are unchanged, blue lines are new borders",
  )+
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(face = "italic", size = "8")
  )

# map of districts and whether they were redrawn
ggplot(data = data, mapping = aes(fill = redrawn)) +
  geom_sf(alpha = 0.9, color = NA) +
  theme_void() +
  labs(title = "Redrawn Precincts",
       fill = "Was the Precinct Redrawn?") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

# Creating a new variable that identifies which gubernatorial candidate received the most votes in each precinct
data_2 <- data %>%
  mutate(candidate = case_when(
    g18govrdes >= g18govdgil & g18govrdes >= g18govoric & g18govrdes >= g18govonpa ~ "g18govrdes",
    g18govdgil >= g18govrdes & g18govdgil >= g18govoric & g18govdgil >= g18govonpa ~ "g18govdgil",
    g18govoric >= g18govrdes & g18govoric >= g18govdgil & g18govoric >= g18govonpa ~ "g18govoric",
    g18govonpa >= g18govrdes & g18govonpa >= g18govdgil & g18govonpa >= g18govoric ~ "g18govonpa",
    TRUE ~ NA_character_
  ))

# Plotting the map
plot1 <- ggplot() +
  geom_sf(data = data_2, aes(fill = candidate), color = NA) +
  theme_void() +
  scale_fill_manual(values = c(g18govrdes = "red", g18govdgil = "blue", g18govoric = "green", g18govonpa = "orange"),
                    labels = c(g18govrdes = "Ron DeSantis (R)", g18govdgil = "Andrew Gillum (D)", g18govoric = "Darcy G. Richardson (R)", g18govonpa = "No Party Affiliation")) +
   labs(title = "Gubernatorial Candidate with most votes in the\n2018 general elections in Florida",
       fill = "Most voted candidate") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 10))

plot1

# Map to show the voting behavior of those precints that were redrawn in 2022
plot2 <- ggplot() +
  geom_sf(data = data_2 %>% filter(redrawn == "TRUE"),
          mapping = aes(fill = candidate), color = NA) +
  theme_void() +
  geom_sf(data = data %>% filter(redrawn == "FALSE"),
          fill = "gray", color = NA) +
  scale_fill_manual(values = c(g18govrdes = "red", g18govdgil = "blue", g18govoric = "green", g18govonpa = "orange"),
                    labels = c(g18govrdes = "Ron DeSantis (R)", g18govdgil = "Andrew Gillum (D)", g18govoric = "Darcy G. Richardson (R)", g18govonpa = "No Party Affiliation")) +
  labs(title = "Gubernatorial Candidate with most votes in the 2018\ngeneral elections in Florida (in redrawn precincts)",
       fill = "Most voted candidate") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 10))

plot2

```

The dispersion of redrawn precincts in Florida does not appear to be random where those in Central Florida were redistricted more than those on the coasts. We suspect underlying political motivations in Congressional redistricting. In order to test this assumption, we deploy a machine learning model with the following predictors: [fill in with variables we settle on]

# Preparing the Dataset for Modeling

```{r}

# checking for class imbalances
summary(data$redrawn)

# select variables for model and change types for optimal use
redrawn_sf = data %>%
  select(pct_std, county, precinct,
         g18ussrsco, g18ussdnel, g18govdgil, g18govrdes,
         redrawn, -geometry) %>%
  mutate_if(is.logical, factor) %>%
  st_drop_geometry()

redrawn_sf

```

# Machine Learning Model 

** SECTION ON MACHINE LEARNING PROCESS **

```{r}
#| warning: false
#| message: false
#| code-fold: true

set.seed(20230510)

# set up v fold cross validation
folds = vfold_cv(redrawn_sf, folds = 10, repeats = 1)

# split precinct data into training and testing data
split = initial_split(redrawn_sf, strata = redrawn)
data_train = training(split)
data_test = testing(split)

# create recipe
redrawn_rec = recipe(redrawn ~ ., data = data_train) %>%
  # update roles of variables to not be used in the model but want to keep
  # for potentiala future use
  update_role(pct_std, county, precinct, new_role = "ID") %>%
  # standardize numeric variables
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  prep()

```

The first machine learning model will be a simple logistic regression in order to set a baseline of performance.

```{r}

# set basic logistic regression model
logistic_mod = logistic_reg(mode = "classification",
                             engine = "glm")

# feed recipe and model to a logistic regression workflow
logistic_wf = workflow() %>%
  add_recipe(redrawn_rec) %>%
  add_model(logistic_mod)

# implement cross validation to evaluate models
logistic_cv = logistic_wf %>%
  fit_resamples(resamples = folds, control = control_resamples(save_pred = TRUE))

# see available metrics
logistic_cv %>% unnest(.metrics)
# show best metrics
logistic_cv %>% collect_metrics()
# pull out best model according to ROC metric
logistic_best_roc = logistic_cv %>%
  select_best(metric = "roc_auc")

# implement final workflow based on best model
logistic_final_wf = finalize_workflow(logistic_wf,
                                       parameters = logistic_best_roc)

```
After having set a baseline of performance with the logistic regression model, implemented with 10-fold cross validation, the next step is to deploy several other models and evaluate their performance relative to this baseline. The following Random Forest model will include the same cross-validation as well as hyperparameter tuning.

```{r}

# set model specifications
rf_tune_spec = rand_forest(mtry = tune(),
            trees = 100,
            min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# feed same recipe and Random Forest model to new workflow
rf_wf = workflow() %>%
  add_recipe(redrawn_rec) %>%
  add_model(rf_tune_spec)

# tune hyperparameters
rf_tune_res = tune_grid(
  rf_wf,
  resamples = folds,
  grid = 20
)

rf_tune_res %>% unnest(.metrics)
rf_tune_res %>% collect_metrics()
rf_tune_res %>% select_best("roc_auc")

# based on best ROC, mtry = 1 and min_n = 28
# create grid for ranges with these values
rf_grid_reg = grid_regular(
  mtry(range = c(0,5)),
  min_n(range = c(20, 40)),
  levels = 5
)

# tune hyperparameters again based on new grid
rf_grid_reg_res = tune_grid(
  rf_wf,
  resamples = folds,
  grid = rf_grid_reg
)

rf_grid_reg_res %>% unnest(.metrics)
rf_grid_reg_res %>% collect_metrics()
# pull out best performing hyperparameters based on ROC metric
rf_best_roc = rf_grid_reg_res %>%
  select_best("roc_auc")

rf_final_mod = finalize_model(
  rf_tune_spec,
  rf_best_roc
)

rf_final_mod %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(redrawn ~ ., data = juice(redrawn_rec) %>%
        select(-pct_std, -county, -precinct)) %>%
  vip::vip(geom = "point")

rf_final_wf = workflow() %>%
  add_recipe(redrawn_rec) %>%
  add_model(rf_final_mod)

rf_final_fit = fit(rf_final_wf, data_train)

```

## Estimate the Out of Sample Error Rate

```{r}

data_test_predictions = predict(rf_final_fit, new_data = data_test)

prediction = bind_cols(
  `actual_redrawn` = data_test$redrawn,
  `predicted_redrawn` = data_test_predictions$.pred_class
)

accuracy(prediction, truth = actual_redrawn, estimate = predicted_redrawn)
recall(prediction, truth = actual_redrawn, estimate = predicted_redrawn)
specificity(prediction, truth = actual_redrawn, estimate = predicted_redrawn)



```

Because Districts 18 and 21 seemed to have the highest predictive power in the model, we dug a little deeper at some of their demographic information to see if we could learn more context for those areas. The following charts show the overall population percentages of major races in the state and then specific population estimates for Districts 18 and 21. 

```{r}
cvap_18 <- st_read("/Users/katieward/Desktop/Georgetown/Spring_2023/Data_Science/ppol670_final/ppol670_final/data/fl_cvap_2018_cd/fl_cvap_2018_cd.shp") 

# Data Visualization of Black population
cvap_18$percent <- (cvap_18$ALL_BLK18 / cvap_18$ALL_TOT18) * 100

black_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percent)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkblue",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "Black Population 2018 by Cong. District",
       fill = "Percentage of District Population") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))


# Data Visualization of Hispanic Population
cvap_18$percenthisp <- (cvap_18$ALL_HSP18 / cvap_18$ALL_TOT18) * 100

hisp_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percenthisp)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkgreen",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "Hispanic Population 2018 by Cong. District",
       fill = "Percentage of District Population") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

hisp_percent

# Data Visualization of White Population

cvap_18$percentwhite <- (cvap_18$ALL_WHT18 / cvap_18$ALL_TOT18) * 100

white_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percentwhite)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkred",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "White Population 2018 by Cong. District",
       fill = "Percentage of District Population") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

white_percent

# Data Vis. of Race Estimates for District 18
cvap_dist18 <- cvap_18 %>%
  filter(DIST=="Congressional District 18 (116th Congress)") %>%
  st_drop_geometry() %>%
  select(ALL_TOT18, ALL_AIA18, ALL_ASN18, ALL_BLK18, ALL_NHP18, ALL_WHT18, ALL_2OM18, ALL_HSP18) %>%
  rename("Total Population Est." = ALL_TOT18, 
          "Native American" = ALL_AIA18, 
         "Black" = ALL_BLK18,
         "Pacific Islander" = ALL_NHP18,
         "White" = ALL_WHT18,
         "Two or More Races" = ALL_2OM18,
         "Hispanic" = ALL_HSP18,
         "Asian" = ALL_ASN18)

cvap_dist18 <- cvap_dist18 %>% 
  pivot_longer(everything(),
                    names_to='Race',
                    values_to='Count')

cvap_dist18$Race <- reorder(cvap_dist18$Race, -cvap_dist18$Count)

dist18_raceplot <- cvap_dist18 %>%
ggplot(aes(x = Race, y = Count, fill= Count)) +
  geom_bar(stat = "identity") +
  labs(title = "Population Estimate of District 18", x = "Race", y = "Count") 

dist18_raceplot

# Data Vis. of Race Estimates for District 21
cvap_dist21 <- cvap_18 %>%
  filter(DIST=="Congressional District 21 (116th Congress)") %>%
  st_drop_geometry() %>%
  select(ALL_TOT18, ALL_AIA18, ALL_ASN18, ALL_BLK18, ALL_NHP18, ALL_WHT18, ALL_2OM18, ALL_HSP18) %>%
  rename("Total Population Est." = ALL_TOT18, 
         "Native American" = ALL_AIA18, 
         "Black" = ALL_BLK18,
         "Pacific Islander" = ALL_NHP18,
         "White" = ALL_WHT18,
         "Two or More Races" = ALL_2OM18,
         "Hispanic" = ALL_HSP18,
         "Asian" = ALL_ASN18)

cvap_dist21 <- cvap_dist21 %>% 
  pivot_longer(everything(),
               names_to='Race',
               values_to='Count')

cvap_dist21$Race <- reorder(cvap_dist21$Race, -cvap_dist21$Count)

dist21_raceplot <- cvap_dist21 %>%
  ggplot(aes(x = Race, y = Count, fill= Count)) +
  geom_bar(stat = "identity") +
  labs(title = "Population Estimate of District 21", x = "Race", y = "Count") 

dist21_raceplot
```

