---
title: "Redistricting Project: Florida"
author: "Ben Burnley, Sean Conner, Gustavo Murillo Velazquez, & Katie Ward"
format: 
  html:
    self-contained: true
---
# Introduction 
```{r}
#| echo: false
#| message: false
#| warning: false
#| output: false

# libraries 
library(tidyverse)
library(tidymodels)
library(spatialsample)
library(sf)
library(patchwork)
library(janitor)
library(mapview)

# data import 
dist_18 = st_read("data/fl_cong_2015_to_2021/fl_cong_2015_to_2021.shp") %>%  # 2015-2021 Congressional District Map 
  clean_names()
dist_22 = st_read("data/fl_cong_adopted_2022/P000C0109.shp") %>%  # 2022 Congressional District Map 
 clean_names() %>% 
  st_transform(crs = st_crs(dist_18))

# voting data at precinct level for 2022
vote_18 = st_read("data/fl_vest_18/fl_vest_18.shp") %>% 
  clean_names() %>% 
  st_transform(crs = st_crs(dist_18))

# filter for valid geometries 
vote_18 = vote_18[st_is_valid(vote_18) == TRUE,]

# transform the crs of dist 20 to match 22 
dist_18 = st_transform(dist_18, crs = st_crs(dist_22))

# check crs 
st_crs(vote_18) == st_crs(dist_18)
st_crs(vote_18) == st_crs(dist_22)
```

Redistricting is a key process in guaranteeing representation in American democracy. Dictated by the Constitution to ensure that Americans are equally represented in the House of Representatives, the decennial process has been part of the politics since the country's beginnings. A by-product of this is that those in power have the ability to draw districts in a way that may favor their party for the coming decade. The most extreme version of this - known as gerrymandering - has a long history in the United States and often brings to mind districts with strange geometric features or no discernible geographic constituency. While this is the most egregious version of what can be done in the redistricting process, large portions of the state are moved into new districts impacting their representation and potentially changing the autonomy of individuals who are redrawn. This project seeks to better understand the redistricting process using spatial analysis and machine learning. We focus explicitly on Florida, a state that received a lot of attention in the most recent redistricting process, where Republicans redrew districts to give themselves four additional Republican-leaning districts and eliminated 3 highly competitive districts. 

When we set out on this process, our initial goal was to compare the geometries of precincts from both the old and new maps to determine what happened to voting patterns when a precinct moved districts. In the process of developing the project, we realized that there is not yet spatial data that includes precinct level results and their respective shapefile. Though this makes our initial goal no longer possible, we opted for another approach: could we predict which districts were redrawn based on the results of the 2018 election? While partisan gerrymandering is not illegal in the United States, political factors being strong predictors of redistricting probability would tell a strong story about the motives and the nature of redistricting. We find that the story is a bit more complex. 


# How to use Redistricting Data Hub's API


The data for this project comes from Redistricting Data Hub, a nonpartisan organization that collects a number of sources of data related to redistricting efforts with the goal of providing resources and facilitating participation in the process. For each state, RDH collects a number of data sets related to district boundaries, election results, voter files, as well as demographics for each state from the American Community Survey. This level of access to a single stash of data makes RDH an ideal source of data for government groups, academics, and anyone interested in the redistricting process to access all the data they need to do analysis. 

To aid in this, the Redistricting Data Hub has an API to locate and download all files related to a given state. We decided this would be the best approach to getting the data necessary for our analysis of Florida's redistricting process. Redistricting Data Hub has a slightly tedious process for accessing the API compared to other sources we have used. First, the individual requesting access must fill out a Google form located on RDH's website. Part of this process is a prompt to verify your identity by emailing a picture of the applicant holding a government issued ID. To complete the process someone at RDH must review your application and verify your identity. This process is more restrictive than most APIs and might be a bit cumbersome for some research applications. 

From here, you have access to a GitHub repository where you can download a Python script (in the code blocks below) to download all the data for a state of interest. Researchers have the ability of downloading up to 4 states at a time, making this process relatively scalable depending upon the scope of your research. We followed these steps to download all files related to the state of Florida in Redistricting Data Hub's database. 

```{python}
#| eval: false


#Import the four libraries needed to run the script. If you do not have these, you may need to install.
import pandas as pd
import requests
import io
from getpass import getpass
import json
import numpy as np}

#Below is the baseurl used to retrieve the list of datasets on the website.
baseurl = 'https://redistrictingdatahub.org/wp-json/download/list'

"""This function retrieves a list of all datasets on the RDH site. In order to run, you must be an API user and registered with the RDH site.
Inputs: username (string), password (string)
Optional Inputs: baseurl"""

def get_list(username, password, states, baseurl=baseurl):
    print('Retrieving list of datasets on RDH Website...')
    if type(states)!=type([]):
        states = [states]
    dfs = []
    for i in states:
        params = {}
        params['username'] = username
        params['password'] = password
        params['format'] = 'csv'
        params['states'] = i
        r = requests.get(baseurl, params=params)
        data = r.content
        try:
            df = pd.read_csv(io.StringIO(data.decode('utf-8')))
        except:
            print('There was an error retrieving the list of datasets, please check that you have the correct password and username')
            return 
        #display(df)
        dfs.append(df)
    df = pd.concat(dfs)
    return df
    
  def check_string(string_list, row):
    if len(string_list)==0:
        return True
    for i in string_list:
        if i not in row:
            return False
    return True
    
  def check_states(state_list, row):
    check_state = []
    if state_list == ['']:
        return True
    else:
        for i in state_list:
            if i == row:
                check_state.append(True)
                return True
            else:
                check_state.append(False)
        if any('True') in check_state:
            return True
        else:
            return False
                
def assign_fullname(state):
    state = state.lower()
    keys = ['al','ak','az','ar','ca','co','ct','de','fl',
              'ga','hi','id','il','in','ia','ks','ky','la','me',
              'md','ma','mi','mn','ms','mo','mt','ne',
              'nv','nh','nj','nm','ny','nc','nd','oh',
              'ok','or','pa','ri','sc','sd','tn','tx',
              'ut','vt','va','wa','wv','wi','wy']
    values = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida',
            'Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine',
            'Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska',
            'Nevada','New Hampshire','New Jersey','New Mexico','New York','North Carolina','North Dakota','Ohio',
            'Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas',
            'Utah','Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming']
    values = [i.lower() for i in values]
    dictionary = dict(zip(keys,values))
    for k, v in dictionary.items():
        if k == state:
            return v
        else:
            continue
    return state
    
def run_state_name(list_of_states):
    new_list = []
    for i in list_of_states:
        state = assign_fullname(i)
        new_list.append(state)
    return new_list
    
def get_inputs(run_no = 1):
    if run_no == 1:
        username = str(input('RDH Username or email: '))
        password = str(getpass(prompt='RDH Password: '))
    print('You can retrieve datasets by state by typing out the full state name or postal code abbreviation (e.g. "Alabama" or "alabama" or "AL" or "al").')
    print('If you would like data for multiple states, please separate by comma (e.g. "Wisconsin, mn").')
    print('Because of the limits of WordPress API, it can only retrieve a list of datasets for one state at a time (since many states have nearly 1,000 datasets), so if you are requesting data from multiple states this step may take several minutes, please be patient.')
    state = str(input('\nWhat state(s) do you want data for? Please separate by comma if multiple. '))
    state = [i.strip() for i in state.split(',')]
    state = [i.lower() for i in state]
    state = run_state_name(state)
    state = [i.lower() for i in state]

    print('\nYou can filter datasets in the state(s) you designated with the criteria listed below. All filter options are case insensitive.')
    print('You may search by year as YYYY for all years from 2010 to 2021.')
    print('You may search by dataset type with the following names: ACS5, CVAP, Projection, election results, voter file, incumbent, disag.')
    print('You may search by geogrpahy with the following: precinct, block, block group, census tract, vtd, county, state, aiannh, zctas, senate districts, legislative districts, congressional districts, house of represenative districts (or other district names for the SLDL or SLDU for a given state -- "districts" will retrieve all district boundaries).')
    print('***Please note that if you would like to retrieve the official redistricting dataset for your state, please use "official" (no quotations) in your query. Not all states will produce an offical dataset.')
    print('You may search by file type as CSV or SHP.')
    string = str(input('\nAny other filtering parameters? Please separate by comma (e.g. "election results, 2016, SHP" etc). '))
    string = [i.strip() for i in string.split(',')]
    string = [i.lower() for i in string]
    
    if run_no ==1:
        inputs = [username,password,state,string]
    else:
        inputs = [state,string]
    return inputs
    
'''This function extracts the data that meets input specifications to the current working directory. In order to run, you must be an API user and registered with the RDH site.
Inputs: username (string), password (string), state_name (string), add_string (list of strings)
Output: N/A'''
def get_data(run_no = 1,inputs=0,df = 0):
    #get list of datasets
    if (run_no == 1) or (run_no>1 and len(inputs)==1):
        inputs = get_inputs()
        username = inputs[0]
        password = inputs[1]
        state_name = inputs[2]
        add_string = inputs[3]
        df = get_list(username,password,state_name)
    else:
        username = inputs[0]
        password = inputs[1]
        inputs = get_inputs(run_no)
        state_name = inputs[0]
        add_string = inputs[1]
        df = get_list(username,password,state_name)
        inputs = [username,password,state_name,add_string]
    #read in the list of data
    for i in df.columns:
        if 'Filter by state found 0 states or unknown states' in i:
            print('*You did not specify the necessary states parameter.*')
            inputs = [inputs[0],inputs[1],'fill','fill']#,df_save]
            return inputs
    if df.shape[0]<10:
        print('\nYou either have an incorrect username/password or you are not a designated API user. To try again, please re-run.')
        print('If you continue to have problems or would like to become an API user, please email info@redistrictingdatahub.org')
        inputs = [0]
        return inputs
    params = {
    'username': username,
    'password': password}
    #subset the df by the additional string info
    df['Title_Format'] = df.apply(lambda x: ' '.join([x['Title'],x['Format']]),axis=1)
    df['Subset'] = df['Title_Format'].apply(lambda x: check_string(add_string,x.lower()))
    df = df[df['Subset']==True].copy()
    #take all of the urls in the subset df and split them to just get the baseurl of the dataset (no params)
    urls = list(df['URL'])
    new_urls = []
    id_dict = {}
    for i in urls:
        print(i)
        new = i.split('?')[0]
        dataset_id = i.split('&datasetid=')[1]
        id_dict.update({new:dataset_id})
        new_urls.append(new)
    titles = list(df['Title_Format'])
    if len(titles) == 0:
        print('\nThere are no datasets that currently meet your criteria. Please re-run with different criteria to extract data.')
        inputs = [inputs[0],inputs[1],'fill','fill']#,df_save]
        return inputs
    else:
        titles = ', '.join(titles)
        print('\nThe datasets to be extracted are: ', titles)
    cont = str(input('\nWould you like to extract these datasets to your current working directory? (Yes/No) '))
    ftype = list(df['Format'])
    data = dict(zip(new_urls,ftype))
    cont = cont.capitalize()
    
    if cont == 'Yes':
        counter = 1
        #iterate over all of the new urls and retrieve the data
        for i in new_urls:
            print('Retrieving', str(counter), 'of',str(len(new_urls)),' files')
            #get the data from the url and the params listed above
            params.update({'datasetid':id_dict.get(i)})
            response = requests.get(i,params)
            #get the file name of the dataset
            file_name = i.split('%2F')[-1]
            file_name = file_name.split('/')[-1]
            file_name_no_zip = file_name.split('.')[0]
            zipdot = '.'+file_name.split('.')[1]
            #because we have multiple datasets with the same name (for CSV and SHP), but we may want SHP or CSV, we need to make them unique filenames
            for k,v in data.items():
                if k == i:
                    dtype = '_'+v.lower()
                else:
                    continue
            #new filename
            if dtype in file_name_no_zip:
                dtype = ''
            file_name = file_name_no_zip+dtype+zipdot
            print('Retrieving ', file_name)
            #write the data
            file = open(file_name, "wb")
            file.write(response.content)
            file.close()
            counter = counter+1
        print('\nDone extracting datasets to current working directory.')
        print('Please re-run to extract additional data.')
    else:
        print('Data was not extracted. Please re-run if you would like to extract data.')
    return inputs
    
def re_run(run_no, inputs):
    run = str(input('\nWould you like to run a new extraction? (Yes/No) '))
    run = run.capitalize()
    if run == 'Yes':
        print('\nStarting a new extraction..')
        run_no = run_no+1
        inputs = get_data(run_no,inputs)
        re_run(run_no,inputs)
    else:
        print('\nThanks for using the RDH API tool! If you want to run again, please re-run the run() function (you will be asked for username/password again).')
        return
        
def check_versions():
    pd_check = str((pd.__version__))=='1.3.1'
    req_check = str(requests.__version__) == '2.25.1'
    np_check = str(np.__version__)=='1.20.3'
    if pd_check == False:
        print('WARNING: You do not have the correct version of pandas to run this script. This script may still work, but you may need to install pandas version 1.3.1 for this script to work.')
    if req_check == False:
        print('WARNING: You do not have the correct version of requests to run this script. This script may still work, but you may need to install requests version 2.25.1 for this script to work.')
    if np_check == False:
        print('WARNING: You do not have the correct version of numpy to run this script. This script may still work, but you may need to install numpy version 1.20.3 for this script to work.')
    
def run():
    check_versions()
    run_no = 1
    inputs = get_data()
    re_run(run_no,inputs)
    
run()
```

# Data Processing 

For our analysis, we ended up primarily relying on three data sets. The first two are the Congressional District maps from Florida's Redistricting process. The first map was utilized for all elections between 2018 and 2020. The second map is the new map that was redrawn for the 2020 redistricting process and used in the 2022 midterms. Last, we used the precinct level voting results from the Voting and Elections Science Team (VEST). During this process we ran into one of the first hurdles in pursuing our ideal project. The plan had been to compare precinct level results across two maps. While precincts are often redrawn with Congressional districts, our aim was to see if a precinct had been drawn into a new district during redistricting, quantify the degree to which this was the case, and then see how this impacted factors like turnout and partisan vote. Unfortunately for our project, no one has joined 2022 midterm results with precinct shapefiles yet and so this approach is not yet feasible. We briefly considered using the only available location data in the 2022 precinct data, polling location name and county, to geocode polling location into point data. This would at least let us compare the district the precinct was within during the 2018 cycle to the district the polling location was in during the 2022 cycle. With only location name for over 6,000 precincts this felt both unfeasible and susceptible to mistakes. Had the data included precinct address, we could have coded point data using `tidygeocoder`.

From here, we decided that the best approach would be to try to say something about the redistricting process given the data we had. The process we agreed on was to attempt to predict whether a precinct was drawn into a new district in the 2022 map based on the 2018 electoral results. To do this, we used the shapefiles for each precinct in the 2018 map and checked to see what Congressional district it was in for the 2018 cycle. We repeated this process for the 2022 Congressional districts map. Using this spatial overlap, we identified the precincts that were moved from one district to another. Of the 6,071 precincts in the 2018 map, 58.7% (3568 precincts) were drawn into a new Congressional district for the 2022 cycle. This total number includes precincts that had been split across districts in 2018, but were completely within a new district in 2022 as well as the opposite scenario where a precinct was split in the 2022 cycle. Combined with the election return results at the precinct level, we now have our outcome variable. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# loop for creating district 2018 objects 
districts_18 <- list()

for(i in 1:27){
districts_18[[i]] <- assign(paste0("dist_18_",i), filter(dist_18, district == i))
}

# loop for creating district 2022 objects
districts_22 <- list()

for(i in 1:28){
districts_22[[i]] <-  assign(paste0("dist_22_", i), filter(dist_22, district == i))
}

# create district columns for 2018 map 
for(i in 1:27){
  assign(paste0("within_18_",i), st_within(vote_18, districts_18[[i]], sparse = FALSE))
}

# create district columns for 2022 map 
for(i in 1:28){
  assign(paste0("within_22_",i), st_within(vote_18, districts_22[[i]], sparse = FALSE))
}

# collect columns
dist18_df = data.frame(dist_1 = within_18_1,
                   dist_2 = within_18_2,
                   dist_3 =  within_18_3,
                   dist_4 = within_18_4,
                   dist_5 = within_18_5,
                   dist_6 = within_18_6,
                   dist_7 = within_18_7,
                   dist_8 = within_18_8,
                   dist_9 = within_18_9,
                   dist_10 = within_18_10,
                   dist_11 = within_18_11,
                   dist_12 = within_18_12,
                   dist_13 = within_18_13,
                   dist_14 = within_18_14,
                   dist_15 = within_18_15,
                   dist_16 = within_18_16,
                   dist_17 = within_18_17,
                   dist_18 = within_18_18,
                   dist_19 = within_18_19,
                   dist_20 = within_18_20,
                   dist_21 = within_18_21,
                   dist_22 = within_18_22,
                   dist_23 = within_18_23,
                   dist_24 = within_18_24,
                   dist_25 = within_18_25,
                   dist_26 = within_18_26,
                   dist_27 = within_18_27)

data_18 = dist18_df %>% 
  mutate(district_18 = case_when(
    dist_1 == TRUE ~ "R",
    dist_2 == TRUE ~ "R",
    dist_3 == TRUE ~ "R", 
    dist_4 == TRUE ~ "R",
    dist_5 == TRUE ~ "D", 
    dist_6 == TRUE ~ "R", 
    dist_7 == TRUE ~ "D", 
    dist_8 == TRUE ~ "R",
    dist_9 == TRUE ~ "D", 
    dist_10 == TRUE ~ "D", 
    dist_11 == TRUE ~ "R", 
    dist_12 == TRUE ~ "R", 
    dist_13 == TRUE ~ "D", 
    dist_14 == TRUE ~ "D", 
    dist_15 == TRUE ~ "R", 
    dist_16 == TRUE ~ "R",
    dist_17 == TRUE ~ "R", 
    dist_18 == TRUE ~ "R", 
    dist_19 == TRUE ~ "R",
    dist_20 == TRUE ~ "D", 
    dist_21 == TRUE ~ "D", 
    dist_22 == TRUE ~ "D",
    dist_23 == TRUE ~ "D", 
    dist_24 == TRUE ~ "D", 
    dist_25 == TRUE ~ "R", 
    dist_26 == TRUE ~ "R",
    dist_27 == TRUE ~ "R", 
    TRUE ~ "Split"
  )) %>% 
  mutate(district_no_18 = case_when(
    dist_1 == TRUE ~ "District 1",
    dist_2 == TRUE ~ "District 2",
    dist_3 == TRUE ~ "District 3", 
    dist_4 == TRUE ~ "District 4",
    dist_5 == TRUE ~ "District 5", 
    dist_6 == TRUE ~ "District 6", 
    dist_7 == TRUE ~ "District 7", 
    dist_8 == TRUE ~ "District 8",
    dist_9 == TRUE ~ "District 9", 
    dist_10 == TRUE ~ "District 10", 
    dist_11 == TRUE ~ "District 11", 
    dist_12 == TRUE ~ "District 12", 
    dist_13 == TRUE ~ "District 13", 
    dist_14 == TRUE ~ "District 14", 
    dist_15 == TRUE ~ "District 15", 
    dist_16 == TRUE ~ "District 16",
    dist_17 == TRUE ~ "District 17", 
    dist_18 == TRUE ~ "District 18", 
    dist_19 == TRUE ~ "District 19",
    dist_20 == TRUE ~ "District 20", 
    dist_21 == TRUE ~ "District 21", 
    dist_22 == TRUE ~ "District 22",
    dist_23 == TRUE ~ "District 23", 
    dist_24 == TRUE ~ "District 24", 
    dist_25 == TRUE ~ "District 25", 
    dist_26 == TRUE ~ "District 26",
    dist_27 == TRUE ~ "District 27", 
    TRUE ~ "Split"
  ))

dist22_df = data.frame(dist_1 = within_22_1,
                   dist_2 = within_22_2,
                   dist_3 =  within_22_3,
                   dist_4 = within_22_4,
                   dist_5 = within_22_5,
                   dist_6 = within_22_6,
                   dist_7 = within_22_7,
                   dist_8 = within_22_8,
                   dist_9 = within_22_9,
                   dist_10 = within_22_10,
                   dist_11 = within_22_11,
                   dist_12 = within_22_12,
                   dist_13 = within_22_13,
                   dist_14 = within_22_14,
                   dist_15 = within_22_15,
                   dist_16 = within_22_16,
                   dist_17 = within_22_17,
                   dist_18 = within_22_18,
                   dist_19 = within_22_19,
                   dist_20 = within_22_20,
                   dist_21 = within_22_21,
                   dist_22 = within_22_22,
                   dist_23 = within_22_23,
                   dist_24 = within_22_24,
                   dist_25 = within_22_25,
                   dist_26 = within_22_26,
                   dist_27 = within_22_27,
                   dist_28 = within_22_28)

data_22 = dist22_df %>% 
  mutate(district_22 = case_when(
    dist_1 == TRUE ~ "R",
    dist_2 == TRUE ~ "R",
    dist_3 == TRUE ~ "R", 
    dist_4 == TRUE ~ "R",
    dist_5 == TRUE ~ "R", 
    dist_6 == TRUE ~ "R", 
    dist_7 == TRUE ~ "R", 
    dist_8 == TRUE ~ "R",
    dist_9 == TRUE ~ "D", 
    dist_10 == TRUE ~ "D", 
    dist_11 == TRUE ~ "R", 
    dist_12 == TRUE ~ "R", 
    dist_13 == TRUE ~ "R", 
    dist_14 == TRUE ~ "D", 
    dist_15 == TRUE ~ "R", 
    dist_16 == TRUE ~ "R",
    dist_17 == TRUE ~ "R", 
    dist_18 == TRUE ~ "R", 
    dist_19 == TRUE ~ "R",
    dist_20 == TRUE ~ "D", 
    dist_21 == TRUE ~ "R", 
    dist_22 == TRUE ~ "D",
    dist_23 == TRUE ~ "D", 
    dist_24 == TRUE ~ "D", 
    dist_25 == TRUE ~ "D", 
    dist_26 == TRUE ~ "R",
    dist_27 == TRUE ~ "R", 
    dist_28 == TRUE ~ "R",
    TRUE ~ "Split"
  ))

# combine datasets and create redrawn column
data_both = data_18 %>% 
  bind_cols(data_22) %>% 
  mutate(redrawn = if_else(district_18 == district_22, FALSE, TRUE)) %>% 
  dplyr::select(district_no_18, district_18, district_22, redrawn)

# data frame with column indicating whether a precinct was drawn into a new district
data = vote_18 %>% 
  bind_cols(data_both)

head(data)
```

# Visualizations 

To get a sense of how redistricting changed over the course of the two cycles, we've produced a number of visualizations. This first visualization shows the changes in the Congressional districts between the two maps. Most notably, you can see how some of the Congressional districts have changed shape particularly in the panhandle. Additionally, Florida gained an additional district after the 2020 Census, so some districts change their number between the two maps. Below that, we have included an overlay of the two maps, with red lines representing old Congressional districts and blue lines representing new Congressional districts. You can see that in certain circumstances, as is the case with District 8, there is essentially no change between maps. Others, like District 18 simply gets renamed in 2022 to be District 21, but does not change its geographic shape. Areas around cities have the most change in terms of where district lines shift. 
```{r}
#| warning: false
#| message: false
#| code-fold: true

# plots comparing two maps 
plot1 = ggplot(dist_18)+
  geom_sf(fill = "white")+
  geom_sf_label(aes(label = district))+
  theme_void()+
  ggtitle("2015-2021 Map")

plot2 = ggplot(dist_22)+
  geom_sf(fill = "white")+
  geom_sf_label(aes(label = district))+
  theme_void()+
  ggtitle("2022 Map")+
  labs(
    caption = "Source: Redistricting Data Hub"
  )

plot1 + plot2

# map of overlapping geometries 
ggplot()+
  geom_sf(data = dist_18, color = "red", alpha = 0, lwd = 1) +
  geom_sf(data = dist_22, color = "blue", alpha = 0, lwd = 1) + 
  theme_void()+
  labs(
    title = "Overlay of Old and New Districts ",
    subtitle = "Red lines are old borders, blue lines are new borders",
  )+
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(face = "italic", size = "8")
  )

```
Precincts are much smaller geographical units and so it helps to have an interactive map like the one below. Some precincts have a large geographic area, but those in urban areas can be as small as several blocks. As you can see, a large number of precincts are split between Congressional districts. The chart below shows whether a precinct was represented by a Democrat, a Republican, or whether the precinct was split between Congressional districts after the 2018 midterm elections.
```{r}
#| warning: false
#| messages: false
#| code-fold: true

# plot of precincts 
mapview(data, zcol = "district_18", col.regions = list("blue", "red", "grey"))
```

The plot below explores our main outcome variable of interest: whether a precinct had change in representation after the redistricting process. Almost 70% of precincts do not experience change in partisan representation after redistricting. There are however 1884 precincts whose representation changes as a result of redistricting and other political factors. These precincts are clustered throughout the state.

```{r}
#| warning: false
#| message: false
#| code-fold: true

# map of districts and whether they were redrawn
ggplot(data = data, mapping = aes(fill = redrawn)) +
  geom_sf(alpha = 0.9, color = NA) +
  theme_void() +
  labs(title = "Redrawn Precincts",
       fill = "Did Precinct Representation Change Parties?",
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

# stats 
data %>% 
  st_drop_geometry() %>% 
  group_by(redrawn) %>% 
  summarize(n = n(), percent = n/6071)
```

We can also explore the relationship between the redistricting process and state level electoral factors such as a precinct's support for Governor Ron Desantis in the 2018 election. The first map shows the results of 2018 Gubernatorial Election by precinct. Ron Desantis does extremely well in suburban and rural areas whereas his challenger. The second plot shows the same results but only shaded in if a district had was drawn into a new district and had a change in representation. Of the 4,187 precincts that did not experience a change in representation, 55% of them voted for Ron Desantis in the 2018 Gubernatorial Election. Of the 1,114 precincts that did have a change in district representation, 47% voted for Desantis. This seems to suggest that Republican districts were slightly less likely to be redrawn, however this difference in probability is relatively small. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# Creating a new variable that identifies which gubernatorial candidate received the most votes in each precinct
data_2 <- data %>%
  mutate(candidate = case_when(
    g18govrdes >= g18govdgil & g18govrdes >= g18govoric & g18govrdes >= g18govonpa ~ "g18govrdes",
    g18govdgil >= g18govrdes & g18govdgil >= g18govoric & g18govdgil >= g18govonpa ~ "g18govdgil",
    g18govoric >= g18govrdes & g18govoric >= g18govdgil & g18govoric >= g18govonpa ~ "g18govoric",
    g18govonpa >= g18govrdes & g18govonpa >= g18govdgil & g18govonpa >= g18govoric ~ "g18govonpa",
    TRUE ~ NA_character_
  ))

# Plotting the map
plot1 <- ggplot() +
  geom_sf(data = data_2, aes(fill = candidate), color = NA) +
  theme_void() +
  scale_fill_manual(values = c(g18govrdes = "red", g18govdgil = "blue", g18govoric = "green", g18govonpa = "orange"),
                    labels = c(g18govrdes = "Ron DeSantis (R)", g18govdgil = "Andrew Gillum (D)", g18govoric = "Darcy G. Richardson (R)", g18govonpa = "No Party Affiliation")) +
   labs(title = "Gubernatorial Candidate with most votes in the\n2018 general elections in Florida",
       fill = "Most voted candidate",
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 10))

plot1

# Map to show the voting behavior of those precincts that were redrawn in 2022
plot2 <- ggplot() +
  geom_sf(data = data_2 %>% filter(redrawn == "TRUE"),
          mapping = aes(fill = candidate), color = NA) +
  theme_void() +
  geom_sf(data = data %>% filter(redrawn == "FALSE"),
          fill = "gray", color = NA) +
  scale_fill_manual(values = c(g18govrdes = "red", g18govdgil = "blue", g18govoric = "green", g18govonpa = "orange"),
                    labels = c(g18govrdes = "Ron DeSantis (R)", g18govdgil = "Andrew Gillum (D)", g18govoric = "Darcy G. Richardson (R)", g18govonpa = "No Party Affiliation")) +
  labs(title = "Gubernatorial Candidate with most votes in the 2018\ngeneral elections in Florida (in redrawn precincts)",
       fill = "Most voted candidate", 
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 10))

plot2

data_2 %>% 
  st_drop_geometry() %>% 
  group_by(redrawn) %>% 
  count(candidate)
```

# Preparing the Dataset for Modeling

```{r}
#| warning: false
#| message: false
#| code-fold: true

# checking for class imbalances
summary(data$redrawn)

# select variables for model and change types for optimal use
redrawn_sf = data %>%
  select(-district_18, -district_22) %>%
  mutate_if(is.logical, factor) %>%
  st_drop_geometry()

```

# Machine Learning Model 

We wanted to see how well the 2018 electoral results at a precinct level predicted whether a precinct was drawn into a new district with different representation. We explored this question using two machine learning models. The code for the process is shown below. We begin by setting up our cross validation using the v-fold method. From there the data is split into training and testing data. For the entire process, we used the popular `tidymodels` package to develop recipes and models for machine learning evaluation. This includes preprocessing steps like identifying non-numeric and categorical variables, addressing imbalances in our outcome variable, normalizing our predictors, and creating a dummy variable for the district to try to account for some geographic factors in the model.
  
```{r}
#| warning: false
#| message: false
#| code-fold: true

set.seed(1234)

# set up v fold cross validation
folds = vfold_cv(redrawn_sf, folds = 10, repeats = 1)

# split precinct data into training and testing data
split = initial_split(redrawn_sf)
data_train = training(split)
data_test = testing(split)

# create recipe
redrawn_rec = recipe(redrawn ~ ., data = data_train) %>%
  # update roles of variables to not be used in the model but want to keep
  # for future reference
  update_role(pct_std, new_role = "ID") %>%
  update_role(county, new_role = "ID") %>%
  update_role(precinct, new_role = "ID") %>%
  # down sample redrawn to address class imbalance
  themis::step_downsample(redrawn) %>%
  # mean-center predictors
  step_normalize(all_numeric_predictors()) %>%
  # dummy split variable
  step_dummy(district_no_18) %>%
  prep()

# see engineered training data
bake(redrawn_rec, new_data = data_train)

```

The first machine learning model will be a simple logistic regression in order to set a baseline of performance. This model will be predicting whether or not a district experienced change in their Congressional representation due to redistricting. This model has an accuracy of 78% with an area under the ROC curve of 85%. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

set.seed(2345)

# set basic logistic regression model
logistic_mod = logistic_reg(mode = "classification",
                            engine = "glm")

# feed recipe and model to a logistic regression workflow
logistic_wf = workflow() %>%
  add_recipe(redrawn_rec) %>%
  add_model(logistic_mod)

# implement cross validation to evaluate models
logistic_cv = logistic_wf %>%
  fit_resamples(resamples = folds, control = control_resamples(save_pred = TRUE))

# show best metrics
logistic_cv %>% collect_metrics()

# pull out best model according to ROC metric
logistic_best_roc = logistic_cv %>%
  select_best(metric = "roc_auc")

# implement final workflow based on best model
logistic_final_wf = finalize_workflow(logistic_wf,
                                      parameters = logistic_best_roc)

# fit final workflow to training data
logistic_fit = logistic_final_wf %>%
  fit(data = data_train)

```

After having set a baseline of performance with the logistic regression model, implemented with 10-fold cross validation, the next step is to deploy another model and evaluate its performance relative to this baseline. The following Random Forest model will include the same cross-validation as well as hyper parameter tuning for the number of predictors used at each split and the minimum number of data points needed for a node split to occur.

```{r}
#| warning: false
#| message: false
#| code-fold: true

set.seed(3456)

# set model specifications
rf_tune_spec = rand_forest(mtry = tune(),
                           trees = 100,
                           min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# feed same recipe and Random Forest model to new workflow
rf_wf = workflow() %>%
  add_recipe(redrawn_rec) %>%
  add_model(rf_tune_spec)

# tune hyper parameters
rf_tune_res = tune_grid(
  rf_wf,
  resamples = folds,
  grid = 20
)

# show best ROC AUC and accuracy
bind_rows(
  rf_tune_res %>% show_best("accuracy", n = 1),
  rf_tune_res %>% show_best("roc_auc", n = 1))

### attempted further hyper parameter tuning but best results were from
### original tune grid

# based on best ROC, mtry = 59 and min_n = 13
# create grid for ranges with these values
# rf_grid_reg = grid_regular(
# mtry(range = c(30,60)),
# min_n(range = c(9,20)),
# levels = 4
# )

# tune hyperparameters again based on new grid
# rf_grid_reg_res = tune_grid(
# rf_wf,
# resamples = folds,
# grid = rf_grid_reg
# )

# show best ROC AUC
# rf_grid_reg_res %>% show_best("roc_auc")

# pull out best performing hyperparameters based on ROC metric
rf_best_roc = select_best(rf_tune_res, "roc_auc")

# finalize workflow for fitting data with parameters from model with best ROC
rf_final_wf = finalize_workflow(rf_wf, parameters = rf_best_roc)

# fit final model to training data and extract parsnip model object
rf_final_fit = fit(rf_final_wf, data = data_train)

```

## Diagnostics on Random Forest Model

The plot below displays the most important variables in predicting whether a precinct had a change in representation following the redistricting process. The most important factor was whether a precinct was split between districts in the 2018 map. The second, third, and sixth most important variables were dummy variables for districts in the Tampa, Jacksonville, and Orlando areas, respectively. Additionally, votes on amendments to ban gambling on dog races, add additional property tax exemptions, and keep legislators from lobbying for pharmaceutical companies in the years following their career in government. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# add best ROC to model parameters
rf_final_mod = finalize_model(
  rf_tune_spec,
  rf_best_roc
)

# visualize variable importance
rf_final_mod %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(redrawn ~ ., data = juice(redrawn_rec) %>%
        select(-pct_std, -county, -precinct)) %>%
  vip::vip(geom = "point") +
  theme_classic() +
  theme(panel.grid.major = element_line())
```

Below are the several diagnostic metrics for the model. The model has 77% accuracy, meaning it correctly classifies the redistricting change in over three-quarters of the cases. The recall of the model was 76%, meaning that of the precincts that experienced a change, the model correctly predicted just over three-quarters of the cases. Lastly, the models specificity, which measures the number of true negatives the model correctly predicted, was 78%. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# create data frame that has whether a district was actually redrawn
# and the model's prediction for whether it was redrawn
prediction = bind_cols(
  `actual_redrawn` = data_test$redrawn,
  `predicted_redrawn` = (predict(rf_final_fit, new_data = data_test))$.pred_class
)

# combine metrics into single data frame
bind_rows(
  accuracy(prediction, truth = actual_redrawn, estimate = predicted_redrawn),
  recall(prediction, truth = actual_redrawn, estimate = predicted_redrawn),
  specificity(prediction, truth = actual_redrawn, estimate = predicted_redrawn),
  precision(prediction, truth = actual_redrawn, estimate = predicted_redrawn)) %>%
  select(.metric, .estimate)

```

The plot below demonstrates the ROC curve for our model. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

# ROC curve 
actual = data_test %>% 
  select(redrawn)
preds = predict(rf_final_fit, new_data = data_test, type = "prob")

results = bind_cols(actual, preds)

results %>% 
  roc_curve(truth = redrawn, .pred_FALSE) %>%
  autoplot()
```

Because Districts 13, 5, and 7 seemed to have the highest predictive power in the model, we dug a little deeper at some of their demographic information to see if we could learn more context for those areas. The following charts show the overall population percentages and demographics for these districts.

This first plot identifies the Congressional districts with the highest Black population in the State. The district at the top of the state, Congressional District 5, has the highest black population in the state. Precincts in this district were very likely to be redrawn in the 2022 map. This district was represented by Democrat Al Lawson in 2018 and has now been split between two Republican represented districts. 

```{r}
#| warning: false
#| message: false
#| code-fold: true

cvap_18 <- st_read("data/fl_cvap_2018_cd/fl_cvap_2018_cd.shp") 

# Data Visualization of Black population
cvap_18$percent <- (cvap_18$ALL_BLK18 / cvap_18$ALL_TOT18) * 100

black_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percent)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkblue",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "Black Population 2018 by Cong. District",
       fill = "Percentage of District Population",
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

black_percent
```
This plot shows the map of Congressional districts with the percent of the population that is Hispanic. 
```{r}
#| warning: false
#| message: false
#| code-fold: true
#| 
# Data Visualization of Hispanic Population
cvap_18$percenthisp <- (cvap_18$ALL_HSP18 / cvap_18$ALL_TOT18) * 100

hisp_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percenthisp)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkgreen",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "Hispanic Population 2018 by Cong. District",
       fill = "Percentage of District Population",
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

hisp_percent
```
This plot shows the map of Congressional districts with the percent of the population that is white.
```{r}
#| warning: false
#| message: false
#| code-fold: true
# Data Visualization of White Population

cvap_18$percentwhite <- (cvap_18$ALL_WHT18 / cvap_18$ALL_TOT18) * 100

white_percent <- ggplot() +
  geom_sf(data = cvap_18, aes(fill = percentwhite)) +
  scale_fill_gradient(name = "Percentage", 
                      low = "white", high = "darkred",
                      breaks = seq(0, 100, by = 10),
                      labels = paste0(seq(0, 100, by = 10), "%")) +
  theme_void() +
  labs(title = "White Population 2018 by Cong. District",
       fill = "Percentage of District Population",
       caption = "Source: Redistricting Data Hub") +
  theme(plot.title = element_text(face = "bold"),
        legend.title = element_text(face = "italic", size = 8))

white_percent
```
Finally, here is the breakdown of demographics for each of top 3 districts identified by our model: District 13 (Tampa), District 5 (Jacksonville/North Florida), and District 7 (North Orlando).
```{r}
# Data Vis. of Race Estimates for District 13
cvap_dist13 <- cvap_18 %>%
  filter(DIST=="Congressional District 13 (116th Congress)") %>%
  st_drop_geometry() %>%
  select(ALL_TOT18, ALL_AIA18, ALL_ASN18, ALL_BLK18, ALL_NHP18, ALL_WHT18, ALL_2OM18, ALL_HSP18) %>%
  rename("Total Population Est." = ALL_TOT18, 
          "Native American" = ALL_AIA18, 
         "Black" = ALL_BLK18,
         "Pacific Islander" = ALL_NHP18,
         "White" = ALL_WHT18,
         "Two or More Races" = ALL_2OM18,
         "Hispanic" = ALL_HSP18,
         "Asian" = ALL_ASN18)

cvap_dist13 <- cvap_dist13 %>% 
  pivot_longer(everything(),
                    names_to='Race',
                    values_to='Count')

cvap_dist13$Race <- reorder(cvap_dist13$Race, -cvap_dist13$Count)

dist13_raceplot <- cvap_dist13 %>%
ggplot(aes(x = Race, y = Count, fill= Count)) +
  geom_bar(stat = "identity") +
  labs(title = "Population Estimate of District 13", x = "Race", y = "Count") 

dist13_raceplot
```
```{r}

# Data Vis. of Race Estimates for District 5
cvap_dist5 <- cvap_18 %>%
  filter(DIST=="Congressional District 5 (116th Congress)") %>%
  st_drop_geometry() %>%
  select(ALL_TOT18, ALL_AIA18, ALL_ASN18, ALL_BLK18, ALL_NHP18, ALL_WHT18, ALL_2OM18, ALL_HSP18) %>%
  rename("Total Population Est." = ALL_TOT18, 
         "Native American" = ALL_AIA18, 
         "Black" = ALL_BLK18,
         "Pacific Islander" = ALL_NHP18,
         "White" = ALL_WHT18,
         "Two or More Races" = ALL_2OM18,
         "Hispanic" = ALL_HSP18,
         "Asian" = ALL_ASN18)

cvap_dist5 <- cvap_dist5 %>% 
  pivot_longer(everything(),
               names_to='Race',
               values_to='Count')

cvap_dist5$Race <- reorder(cvap_dist5$Race, -cvap_dist5$Count)

dist5_raceplot <- cvap_dist5 %>%
  ggplot(aes(x = Race, y = Count, fill= Count)) +
  geom_bar(stat = "identity") +
  labs(title = "Population Estimate of District 5", x = "Race", y = "Count") 

dist5_raceplot
```
```{r}
# Data Vis. of Race Estimates for District 5
cvap_dist7 <- cvap_18 %>%
  filter(DIST=="Congressional District 7 (116th Congress)") %>%
  st_drop_geometry() %>%
  select(ALL_TOT18, ALL_AIA18, ALL_ASN18, ALL_BLK18, ALL_NHP18, ALL_WHT18, ALL_2OM18, ALL_HSP18) %>%
  rename("Total Population Est." = ALL_TOT18, 
         "Native American" = ALL_AIA18, 
         "Black" = ALL_BLK18,
         "Pacific Islander" = ALL_NHP18,
         "White" = ALL_WHT18,
         "Two or More Races" = ALL_2OM18,
         "Hispanic" = ALL_HSP18,
         "Asian" = ALL_ASN18)

cvap_dist7 <- cvap_dist7 %>% 
  pivot_longer(everything(),
               names_to='Race',
               values_to='Count')

cvap_dist7$Race <- reorder(cvap_dist7$Race, -cvap_dist7$Count)

dist7_raceplot <- cvap_dist7 %>%
  ggplot(aes(x = Race, y = Count, fill= Count)) +
  geom_bar(stat = "identity") +
  labs(title = "Population Estimate of District 7", x = "Race", y = "Count") 

dist7_raceplot
```

